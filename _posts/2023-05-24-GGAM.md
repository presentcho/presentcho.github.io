---
title: '[Paper Reading] Estimation and Inference for Generalized Geoadditive Models'
date: 2023-05-24 20:20:00 +0000
categories: [PaperReading, GAM]
math: true
---

## Introduction

I would like to introduce a paper 'Estimation and Inference for Generalized Geoadditive Models' by Shan Yu. The resource is [here](https://www.tandfonline.com/doi/abs/10.1080/01621459.2019.1574584?journalCode=uasa20). This paper proposes generalized geoadditive models (GGAMs). The univariate additive component is approximated by univariate polynomial splines, and the geographical component is approximated by the bivariate penalized splines over triangulation.

## Model

The conditional density of \(Y\) given \((X,S) = (x,s)\) belongs to the exponential family \(f_{Y\|X,S}(y\|x,s) = \exp[y\xi(x,s)- \mathbf{B}\{\xi(x,s)\} + \mathbf{C}(y)]\), where \(\xi\) is the natural parameter.

The unknown mean response \(\mu(x,s) = E(Y\|X=x, S=s) = \mathbf{B}'\{\xi(x,s)\}\)

In this paper, the link function \(g\{\mu(x,s)\} = \sum_{k=1}^{p}\beta_k(x_k) + \alpha(s)\), where \(\beta\) is unknown univariate smooth functions, and \(\alpha\) is an unknown bivariate smooth function.

## Approximation

1. Univariate polynomial spline approximation

Let \(v_k\) be a partition of \([a_k, b_k]\) with \(J_n\) interior knots, and \(v_k = \{a_k = v_{k,0} < v_{k,1}< ... < v_{k,J_n} < v_{k,J_{n+1}} = b_k\}\`

2. Penalized Quasi-Likelihood Estimators

We use penalized quasi-likelihood estimator to fit data-sparse regions, and we would like to minimize:

\[
-\sum_{i=1}^{n}\ell[g^{-1}\{\sum_{k=1}^{p}U_k(X_{ik})^{\top}\theta_k + \mathbf{B}(S_i)^{\top}\gamma\}, Y_i] + \frac{1}{2}\lambda\gamma^{\top}\mathbf{P}\gamma
\]

subject to \(\Psi\gamma = 0\),

where \(\mathbf{P}\) is the block diagonal penalty matrix, we use QR decomposition to satisfy the smoothness condition.

\[
\mathbf{L}^{p}(\theta, \gamma^{\ast}) = -\sum_{i=1}^{n}\ell[g^{-1}\{\sum_{k=1}^{p}U_k(X_{ik})^{\top}\theta_k + \mathbf{B}(S_i)^{\top}Q_2\gamma^{\ast}\}, Y_i] + \frac{1}{2}\lambda\gamma^{*\top}\mathbf{Q}_2^{\top}\mathbf{P}\mathbf{Q}_2\gamma
\]

Then the univariate spline estimator and bivariate spline estimator are 
\(\hat{\beta_k}(x_k) = \mathbf{U}_k^{\top}(x_k)\hat{\theta}_k\), \(\hat{\alpha}(s) = \mathbf{B}(s)^{\top}\hat{\gamma}\), 

where \(\hat{\gamma} = \{\hat{\gamma}_m, m \in \mathbf{M}\}^{\top} = \mathbf{Q}_2\hat{\gamma}^{\ast}\`

What is \(\mathbf{U}\) and why do we need this? Based on the definition from the paper, we centralize \(u_{kj}^{0}(x_k)\) because it makes the model identifiable.

To be more specific, \(\sum_{k=1}^{p}{\beta_{k}(x_k) + c_k} + \alpha(s) - \sum_{k=1}^{p}c_k \Rightarrow \sum_{k=1}^{p}\tilde{\beta}_k(x_k) + \tilde{\alpha}(s)\), if we don't have a constraint \(E(\beta_k(x)) = 0\) then we have the same \(\mu\) and it is not estimable.

Once we centralize \(u_{kj}\), we can satisfy the constraint automatically. Thus, we need to centralize to constrain \(\beta_k(x_k)\).

## Theorem 1

\[
\sum_{k=1}^{p} \|\hat{\beta}_k - \beta_k\|_{L_2} + \|\hat{\alpha}-\alpha\|_{L_2}
= O_{a.s}\{(H^{-1/2} + \|\triangle\|^{-1})(\log n/n)^{1/2}+ H^{\rho+1} +  \|\triangle\|^{d+1} + \frac{\lambda}{n\|\triangle\|^{4}}\}
\]

Here, we would like to compare the estimated coefficients and true coefficients. The interpretation of this theorem would be \((H^{-1/2} + \|\triangle\|^{-1})(\log n/n)^{1/2}\) is a variance term, \(H^{\rho+1}\) is an approximated power for univariate spline bias term, \(\|\triangle\|^{d+1}\) is an approximated power for bivariate spline bias term, and \(\frac{\lambda}{n\|\triangle\|^{4}}\) is a penalty term. When we are trying to approximate the lower dimension, we will have the poor-fit and we use these bias terms to adjust.

**Proof:** We need lemma B.2 and it shows that \(\theta_{kj}^{2}\) is bounded by \(L_2\). Similarly, \(\gamma_{m}^{2}\) is bounded by \(L_2\).

...

