---
title: '[Paper Reading] Estimation and Inference for Generalized Geoadditive Models'
date: 2023-05-24 20:20:00 +0000
categories: [PaperReading, GAM]
math: true
---

## Introduction

I would like to introduce a paper 'Estimation and Inference for Generalized Geoadditive Models' by Shan Yu. The resource is [here](https://www.tandfonline.com/doi/abs/10.1080/01621459.2019.1574584?journalCode=uasa20). This paper proposes generalized geoadditive models (GGAMs). The univariate additive component is approximated by univariate polynomial splines, and the geographical component is approximated by the bivariate penalized splines over triangulation.

## Model 

The conditional density of $Y$ given $(X,S) = (x,s)$ belongs to exponential family 
$f_{Y||X,S}(y||x,s) = exp\[y \xi(x,s)- \mathbf{B}{\xi(x,s)} + \mathbf{C}(y)\]$, where $\xi$ is natural parameter.


The unknown mean response is 

$$\mu (x,s) = E(Y || X=x, S=s)$  $= \mathbf{B}^{'} \{ \xi (x,s) \}$$


In this paper, the link function $g\{\mu (x,s)\} = \sum_{k=1}^{p}\beta_k(x_k) + \alpha(s)$,where $\beta$ is unknown univariate smooth functions and $\alpha$ is unknown bivariate smooth function.

## Approximation 

1. Univariate polynomial spline approximation

Let $v_k$ is a partition of $[a_k, b_k]$ with $J_n$ interior knots and $v_k = {a_k = v_{k,0} < v_{k,1}< ... < v_{k,J_n} < v_{k,J_{n+1}} = b_k}$

2. Penalized Quasi-Likelihood Estimators

We use penalized quasi-likelihood estimator to fit data-sparse regions and we would like to minimize:

$$-\sum_{i=1}^{n}\ell[g^{-1}\{\sum_{k=1}^{p}U_k(X_{ik})^{\top}\theta_k + \mathbf{B}(S_i)^{\top}\gamma\}, Y_i] + \frac{1}{2}\lambda\gamma^{\top}\mathbf{P}\gamma$$ subject to $\Psi\gamma = 0$,

where $\mathbf{P}$ is the block diagonal penalty matrix, we use QR decomposition to satisfy the smoothness condition. 

$$\mathbf{L}^{p}(\theta, \gamma^{\ast}) = -\sum_{i=1}^{n}\ell[g^{-1}\{\sum_{k=1}^{p}U_k(X_{ik})^{\top}\theta_k + \mathbf{B}(S_i)^{\top}Q_2\gamma^{\ast}\}, Y_i] + \frac{1}{2}\lambda\gamma^{*\top}\mathbf{Q}_2^{\top}\mathbf{P}\mathbf{Q}_2\gamma$$

Then the univariate spline estimator and bivariate spline estimator are 
$\hat{\beta_k}(x_k) = \mathbf{U}_k^{\top}(x_k)\hat{\theta}_k$, $\hat{\alpha}(s) = \mathbf{B}(s)^{\top}\hat{\gamma}$, where $\hat{\gamma} = \{\hat{\gamma}_m, m \in \mathbf{M}\}^{\top} = \mathbf{Q}_2\hat{\gamma}^{\ast}$

*What is $\mathbf{U}$ and why do we need this?* Based on the definition from the paper, we centralize $u_{kj}^{0}(x_k)$ because it makes the model identifiable.

To be more specific, $\sum_{k=1}^{p}{\beta_{k}(x_k) + c_k} + \alpha(s) - \sum_{k=1}^{p}c_k \Rightarrow \sum_{k=1}^{p}\tilde{\beta}_k(x_k) + \tilde{\alpha}(s)$, if we don't have a constraint $E(\beta_k(x)) = 0$ then we have the same $\mu$ and it is not estimable.

Once we centralize $u_{kj}$, we can satisfy the constraint automatically. Thus, we need to centralize to constrain $\beta_k(x_k)$.

## Theorem 1

$$\sum_{k=1}^{p} ||\hat{\beta}_k - \beta_k||_{L_2} + ||\hat{\alpha}-\alpha||_{L_2}$$

$$= O_{a.s}\{(H^{-1/2} + ||\triangle||^{-1})(logn/n)^{1/2}+ H^{\rho+1} +  ||\triangle||^{d+1} + \frac{\lambda}{n||\triangle||^{4}}\}$$

Here, we would like to compare the estimated coefficients and true coefficients. 
The interpretation of this theorem would be 

$(H^{-1/2} + || \triangle ||^{-1})(logn/n)^{1/2}$ is variance term, 

$H^{\rho+1}$ is approximated power for univariate spline bias term, 

$ || \triangle ||^{d+1}$ is approximated power for bivariate spline bias term, 

and $\frac{\lambda}{n || \triangle ||^{4}}$ is a penalty term. 

When we are trying to approximate the lower dimension, we will have the poor-fit and we use these bias terms to adjust. 

*Proof)* We need lemma B.2 and it shows that $\theta_{kj}^{2}$ is bounded by $L_2$. Similarly, $\gamma_{m}^{2}$ is bounded by $L_2$.  


## Proof Study

### Preliminary result

$P(lim inf A_n) \leq liminf P(A_n) \leq limsup P(A_n) \leq P(limsup A_n)$

**Borel Cantelli lemma**.  Let $A_1, ..., \in \mathcal{F}$
1) If $\sum_n P(A_n) < \infty$, then $P(limsup_n A_n) = 0$
2) If $\sum_n P(A_n) = \infty$ and $\{A_n\}$ are independent, then $P(linsup_nA_n) = 1$

**Fatou's lemma** If $X_n \geq C$ for all $n$, and some constant $C > - \infty$, then 

$$E(lim inf_{n \rightarrow \infty} X_n) \leq lim inf_{n \rightarrow \infty} E(X_n)$$

Let $\{\xi_{in} \}^{n}_{i=1}$ be independent r.v.s $E(\xi_{in}^{2}) = \sigma^{2}_{in} > 0$, $E(\xi_{in})=0$. Denote $S_n = \sum_{i=1}^{n}\xi_{in}$, $V_n = \sum_{i=1}^{n}\sigma_{in}^{2}$
 
 **CLT** 
 $$V_n^{-1}S_n \rightarrow N(0,1)$$

**Lindeberg condition**

$$\text{For } \varepsilon > 0, V_n^{-2} \sum_{i=1}^{n}E\xi_{in}^{2}I\{ |\xi_{in}| \geq \varepsilon V_n\} \rightarrow 0$$

**Lyapunov condition**

$$\text{For } \sigma> 0, V_{n}^{-(2+\sigma)}\sum_{i=1}^{n}E||\xi_{in}||^{2+\sigma} \rightarrow 0$$

**Cramer's condition** 

There exists a constant $c > 0$ such that 

$$E||\xi_{in}||^{k} \leq c^{k-2}k! E\xi_{in}^{2} < +\infty, i = 1,...,n, k = 3,4,...$$

**Bernstein's Inequality** gives bounds on the probability that the sum of random variables deviates from its mean. (Cramer's condition needs to be satisfied to use Bernstein's inequality and Bernstein inequality is commonly used to prove asymptotic properties in spline model)

$$P(|\frac{S_n}{V_n}| \geq t) \leq 2 exp\{-\frac{t^2}{4}\frac{1}{1+ct V_n^{-1}/2}\}$$

**Lemma B.3** By assmption A2 and A5

$$c(\sum_{k=1}^{p} \sum_{j \in J} \theta^{2}_{kj}+ \sum_{m \in \mathcal{M}} \gamma_m^{2})$$

$$\leq || \sum_{k=1}^{p}\sum_{j \in J} \theta_{kj}U_{kj}+ \sum_{m \in \mathcal{M}}\gamma_m B_{m}^{\ast}||^{2}$$

$$\leq C(\sum_{k=1}^{p}\sum_{j \in J}\theta^{2}_{kj}+ \sum_{m \in \mathcal{M} } \gamma_{m}^{2})$$


Recall that $EU_{kj}(X_k) = 0$, then 


$$ ||\sum_{k=1}^{p}\sum_{j \in J}\theta_{kj}U_{kj} + \sum_{m \in \mathcal{M}}\gamma_m B_{m}^{\ast}||^{2} $$


$$= || \sum_{k=1}^{p}\sum_{j \in J}\theta_{kj}U_{kj} + \sum_{m \in \mathcal{M}}\gamma_m (B_{m}^{\ast}- \mu_m) + \sum_{m \in \mathcal{M}}\gamma_m \mu_m||^{2}$$


Let $A = \sum_{k=1}^{p}\sum_{j \in J}\theta_{kj}U_{kj} + \sum_{m \in \mathcal{M}}\gamma_m (B_{m}^{\ast}- \mu_m)$ and $B = \sum_{m \in \mathcal{M}}\gamma_m \mu_m$


$$E||A+B||^{2} = ||A||^{2} + ||B||^{2} + 2E(AB) \text{where } E(A) = 0$$


Thus, $$ || \sum_{k=1}^{p} \sum_{j \in J} \theta_{kj} U_{kj} +  \sum_{m \in \mathcal{M}} \gamma_m B_{m}^{\ast} ||^{2}$$


$$= || \sum_{k=1}^{p} \sum_{j \in J}\theta_{kj}U_{kj} + \sum_{m \in \mathcal{M}}\gamma_m (B_{m}^{\ast} - \mu_m)||^{2} + ||\sum_{m \in \mathcal{M}}\gamma_m \mu_m||^{2}$$


**Lemme B.6** 

$$max |\frac{1}{n} \sum_{i=1}^{n} U_{kj} (X_{ik}) U_{k^{'}j^{'}}(X_{ik^{'}}) - E\{U_{kj}(X_{ik})U_{k^{'}j^{'}}(X_{ik^{'}}\} | = O_{a.s}(n^{-1/2}H^{-1/2}log^{1/2}n)$$

$$max |\frac{1}{n}\sum_{i=1}^{n}B^{\ast}_{m}(S_i)B^{\ast}_{m^{'}}(S_i) - E\{B^{\ast}_{m}(S_i)B^{\ast}_{m^{'}}(S_i) \} | = O_{a.s}(n^{-1/2}||\triangle||^{-1/2}log^{1/2}n)$$

$$max |\frac{1}{n}\sum_{i=1}^{n}B^{\ast}_{m}(S_i)U_{kj}(X_{ik}) - E\{B^{\ast}_{m}(S_i)U_{kj}(X_{ik}) \} | = O_{a.s}(n^{-1/2}log^{1/2}n)$$

**Lemma B.7** Suppose that assumptions (A2), (A5), and (A6). 

$$R_n = sup |\frac{<\psi_1, \psi_2>_n - <\psi_1, \psi_2>}{|||\psi_1||||||\psi_2|||} | = O_{a.s.}\{H^{-1/2} |\triangle |^{-1}n^{-1/2}log^{1/2}n\}$$

$$|\psi_1 | = \sum_{k=1}^{p}\sum_{j \in J}\theta_{kj,1}U_{kj} + \sum_{m \in \mathcal{M}}\gamma_{m,1}B_{m}^{\ast}$$
