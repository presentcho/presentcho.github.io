---
title: '[Linear Models] Ordinary Least Squares Regression (OLS)'
date: 2023-05-23 15:20:00 +0000
categories: [LinearModels]
math: true
---

# Introduction

OLS regression find a linear function of $X$ that minimizes the sum of squared residuals from $Y$

[figure]

We would like to find the beta $argmin_{\beta}(y-X\beta)'(y-X\beta)$

## OLS Properties
1. When we have assumptions for $\varepsilon$ error term, it's MLE

2. Gauss Markow theorem states that $\hat{\beta}$ is BLUE (best linear unbiased estimator)

## OLS Residual Properties
1. $E(y) = E(\hat{y}) = \bar{y}$ and the sum of residuals is 0 

2. $\hat{y}$ and $\varepsilon$ is uncorrelated (cov(\hat{y}, \varepsilon) = 0)

## 1. Normal Equation 

$X'y = X'X\beta$

## 2. Estimator

$\hat{\beta} = (X'X)^{-1}X'y$

$E(\hat{\beta} = \beta$

$Cov(\hat{\beta}) = \sigma^2(X'X)^{-1}$

## 3. Residuals Sum of Square and Variance Estimation

$RSS = y'(I-P_x)y$


$E(RSS) = \sigma^2 tr(I-P_x) = \sigma^2 (n-p-1)$

$\hat{\sigma}^2 = \frac{RSS}{n-p-1} = \frac{y'(I-P_x)y}{n-p-1}$ 

$E(\hat{\sigma}^2) = \sigma^2$, which means $\hat{\sigma}^2$ is an unbiased estimator for $\sigma^2$

$\hat{\sigma}^2 = \frac{RSS}{n-p-1} = \frac{y'(I-P_x)y}{n-p-1} = \frac{(X\beta + \varepsilon)'((I-P_x))(X\beta + \varepsilon)}{n-p-1} = \frac{\varepsilon'(I-P_x) \varepsilon}{n-p-1}$


$\Leftrightarrow \frac{(n-p-1)\hat{\sigma}^2}{\sigma^2} = \frac{\varepsilon'(I-P_x) \varepsilon}{\sigma^2} \sim \chi^{2}_{n-p-1}$





