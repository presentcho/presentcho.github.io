# 🚀 Why Optimization Algorithms Are Needed

## 🧮 Goal of Logistic Regression

The goal of logistic regression is to find the **model parameters (β)** that minimize the **cost function**. The cost function typically used is the **Log-Loss function**:

$$
J(\beta) = -\frac{1}{m}\sum_{i=1}^m \left[y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)\right]
$$

- **m**: Number of samples
- **y_i**: Actual label
- **\hat{y}_i**: Predicted probability

---

## 💡 Role of Optimization Algorithms

Optimization algorithms update the parameter \(\beta\) to minimize this cost function.

### 🔍 Common Optimization Algorithms

1. **Gradient Descent**

- At each iteration, the algorithm computes the **gradient** of the cost function and updates the parameters in the direction of the negative gradient.

$$
\beta = \beta - \alpha \frac{\partial J(\beta)}{\partial \beta}
$$

- **\(\alpha\)**: Learning rate

2. **Advanced Solvers**

- **'lbfgs'**: Suitable for multivariable optimization, performs well with small datasets.
- **'sag' (Stochastic Average Gradient)**: Ideal for large datasets.
- **'saga'**: Supports L1 and ElasticNet penalties, handles large datasets.
- **'newton-cg'**: Good for multivariable optimization and multiclass logistic regression.
- **'liblinear'**: Best for small datasets, supports L1 and L2 penalties.

---

### ⚠️ Why Do We Need Multiple Algorithms?

- Performance varies depending on the **size and characteristics of the data**.
- **Large datasets:** **sag** and **saga** are preferable.
- **Small datasets:** **liblinear** and **lbfgs** are often better.
- For L1 regularization, use **saga** or **liblinear** solvers.

---

# 🧮 Role of Penalty Term and λ (Lambda)

## 📌 Why Add a Penalty Term?

A **penalty term** is added to logistic regression to prevent **overfitting** by reducing the complexity of the model.

### 💡 Types of Penalty Terms

1. **L2 Penalty (Ridge Regression)**

$$
J(\beta) = -\text{Log-Loss} + \lambda \sum_{j=1}^p \beta_j^2
$$

- Shrinks all **coefficients (β)**, reducing model complexity.
- The higher the \(\lambda\), the stronger the regularization, and coefficients approach **0**.

2. **L1 Penalty (Lasso Regression)**

$$
J(\beta) = -\text{Log-Loss} + \lambda \sum_{j=1}^p |\beta_j|
$$

- Sets some coefficients to **exactly 0**, enabling **feature selection**.

3. **ElasticNet (Combination of L1 + L2)**

$$
J(\beta) = -\text{Log-Loss} + \lambda_1 \sum_{j=1}^p |\beta_j| + \lambda_2 \sum_{j=1}^p \beta_j^2
$$

- Combines the strengths of **L1 and L2 regularization**.
- The **l1_ratio** parameter adjusts the balance between L1 and L2 penalties.

---

## Role of λ (Lambda)

- When \(\lambda\) is **large**, the regularization is **strong**:
  - Reduces **overfitting**, but increases the risk of **underfitting**.

- When \(\lambda\) is **small**, the regularization is **weak**:
  - Increases the risk of the model **overfitting** to the data.

